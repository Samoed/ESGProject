import pandas as pd
import numpy as np
from collections import defaultdict


df = pd.read_csv("paragrapghs_with_topisc.csv.zip", index_col=0)
df.head()


df.loc[df["company"] == "reports/ПАО «ВЫМПЕЛКОМ» ", ["company"]] = "reports/ПАО «ВЫМПЕЛКОМ»"


not_nan = df[df["max_topic_exp"].notna()]
# число абзацев соотвествующих теме
reports = not_nan.groupby(["company", "index", "max_topic_exp"])["year"].count()
reports.head()








topics_per_report = not_nan.groupby("index")["year"].count()
topics_per_report.head()


tf = defaultdict(dict)
for (file_name, index, topic), row in reports.items():
    tf[index][topic] = row / topics_per_report[index]
tf = pd.DataFrame(tf).T





idf = {}
for topic in df["max_topic_exp"].unique():
    if pd.isna(topic):
        continue
    idf[topic] = np.log(df["index"].nunique() / tf[topic].notna().sum())
idf = pd.Series(idf)


tf_idf = defaultdict(dict)
for idx, row in tf.iterrows():
    for topic in row.index:
        tf_idf[idx][topic] = row[topic] * idf[topic]


tf_idf = pd.DataFrame(tf_idf).T
tf_idf = tf_idf.fillna(0)
tf_idf.head()


tf_idf.to_csv("tf_idf.csv")


raex_scores = pd.read_csv("raex_esg_with_rspp.csv", index_col=0)
raex_scores.head()


tf_idf


assert len(tf_idf.index[~tf_idf.index.isin(raex_scores.index)]) == 0, (
    tf_idf.index[~tf_idf.index.isin(raex_scores["компания"])],
    raex_scores[~raex_scores["компания"].isin(tf_idf.index)]["компания"].unique(),
)


combined = tf_idf.join(raex_scores[["E_transformed", "S_transformed", "G_transformed"]])


combined.to_csv("tfidf_with_esg.csv")


topic_words = pd.read_csv("../topic_words.csv", index_col=0)
topic_words.head()


gr = topic_words.groupby("meta")["topic"].apply(set)


topics = set([col.split("_")[0] for col in combined.columns if col.find("_") != -1])
topics = [topic for topic in topics if len(topic) > 1]


for (
    let,
    row,
) in gr.items():
    print(let, len(row))


letter = {"env": ["E_transformed"], "social": ["S_transformed"], "gov": ["G_transformed"]}
for meta, col_set in gr.items():
    for col in col_set:
        found = False
        for c_col in combined.columns:
            if col in c_col:
                letter[meta].append(c_col)
                found = True
        # if not found:
        #     print(col)
for key, arr in letter.items():
    print(key, len(arr))





from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score


def logreg_(data: pd.DataFrame, cur_letter: str):
    model = LogisticRegression()
    y_col = f"{cur_letter}_transformed"
    X = data.drop([y_col], axis=1)
    y = data[y_col]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    print(f"{X_train.shape=} {X_test.shape=}")

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print("accuracy_score", accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred))


def map_to_ranges(value):
    if value <= 3:
        return 1
    elif value <= 6:
        return 2
    else:
        return 3


for cur_letter in ["E", "S", "G"]:
    col = f"{cur_letter}_transformed"
    combined[col] = combined[col].apply(map_to_ranges)


combined.head()


for cur_letter, letter_type in [("E", "env"), ("S", "social"), ("G", "gov")]:
    print(cur_letter, letter_type)
    logreg_(combined[letter[letter_type]], cur_letter)


combined["E_transformed"].value_counts()





from pymcdm.methods import TOPSIS


gr = (
    df.drop(
        columns=[
            "Unnamed: 0",
            "rsspp_index",
            "sector",
            "year",
            "report_type",
            "paragraph",
            "original_text",
            "cleaned_text",
            "max_topic_cos",
            "max_score_cos",
            "count_words",
            "second_max_topic_cos",
            "second_max_score_cos",
            "max_topic_exp",
            "max_score_exp",
            "second_max_topic_exp",
            "second_max_score_exp",
        ]
    )
    .groupby(["company", "index"])
    .mean()
    .dropna(how="all")
)
gr.head()


topics = topic_words.groupby("meta")["topic"].apply(set)


gr.reset_index(inplace=True)
gr = gr.set_index("index")
gr.head()


def second_workflow(data: pd.DataFrame):
    matrix = data.to_numpy()
    weights = np.array([1 / matrix.shape[0]] * matrix.shape[1])
    types = np.ones(matrix.shape[1], dtype=int)
    body = TOPSIS()
    predictions = [round(preference, 2) for preference in body(matrix, weights, types)]
    return predictions


gr = gr.join(raex_scores["Название"])


gr["company"].unique()


bad = ["Кибербезопасность", "Инновации", "Персонал в целом"]
gr["company"] = gr["company"].str.split("/").str[1]
for key, cur_topics in topics.items():
    tmp_topics = []
    for t in cur_topics:
        if t not in bad:
            tmp_topics.append(t)
        else:
            tmp_topics.append("!" + t)
    cur_topics = tmp_topics
    res = second_workflow(gr[cur_topics])
    pd.Series(res).set_axis(gr.index).to_csv(f"{key}.csv")
    gr[key] = res


bad = ["Кибербезопасность", "Инновации", "Персонал в целом"]

tmp_topics = []
for t in [v for _, val in topics.items() for v in val]:
    if t not in bad:
        tmp_topics.append(t)
    else:
        tmp_topics.append("!" + t)
cur_topics = tmp_topics
res = second_workflow(gr[cur_topics])
res = pd.Series(res).set_axis(gr.index)
gr["all_letters"] = res


gr["on_letters"] = second_workflow(gr[["env", "gov", "social"]])
gr.to_csv("raex_results.csv")


df = pd.read_csv("raex_results.csv", index_col=0)


df.tail()



